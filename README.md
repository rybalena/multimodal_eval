# AI MultimodalEval  
ğŸ“Š **Evaluation & QA Framework for Multimodal AI (Visionâ€“Language Models)**  

---

## ğŸ” What is MultimodalEval?

**MultimodalEvalCore** is an open-source framework for **evaluating multimodal AI models** (image â†’ text).  
It is designed for **AI QA engineers, ML researchers, and practitioners** who need **reliable, reproducible, and extensible** benchmarks.  

Supports **Captioning, VQA, Hallucination Detection, and Contextual Relevance**, combining **classic metrics** (CIDEr, CLIPScore, SBERT) with **LLM-as-a-Judge** scoring.  

---

## âš™ï¸ Features

- ğŸ–¼ **Tasks**: Captioning Â· VQA Â· Hallucination Detection Â· Contextual Relevance  
- ğŸ“Š **Metrics**: CLIPScore Â· CIDEr Â· Semantic Similarity Â· Hybrid Hallucination Â· Composite Score  
- ğŸ”Œ **Models**: LLaVA (via Ollama) Â· Qwen2-VL Â· OpenAI GPT-4o  
- ğŸ–¥ **Interfaces**:  
  - CLI (`cli/main.py`, `cli/run_all.py`)  
  - Streamlit UI (`app_ui/streamlit_ui/streamlit_app.py`)  
  - Results Viewer (`app_ui/results_viewer/results_viewer.py`)  
  - Python API (`Evaluator`, `evaluate_sample`)  
- ğŸ“‚ **Results**: JSON + CSV tables with metrics and composite scores  
- ğŸ§ª **Testing**: PyTest, fixtures, coverage reports  
- ğŸ“¥ **Custom datasets**: Add your own datasets via `--sources user` or `add_dataset`  
- ğŸ›  **Docs**: Full user guides for installation, usage, tasks, metrics, results, tests, troubleshooting, roadmap  

---

## ğŸ“ Project Structure

```
multimodal_eval/
â”œâ”€â”€ cli/                 # CLI entrypoints (main.py, run_all.py)
â”œâ”€â”€ datasets/            # Labeled & Unlabeled datasets
â”œâ”€â”€ evaluation/          # Metrics (clip, cider, hallucination, composite, etc.)
â”œâ”€â”€ model_wrappers/      # LLaVA, Qwen, GPT-4o integration
â”œâ”€â”€ app_ui/              # Streamlit UI + Results Viewer
â”œâ”€â”€ configs/             # Task configs, prompts
â”œâ”€â”€ results/             # Saved JSON/CSV evaluation outputs
â”œâ”€â”€ tests/               # Unit & integration tests
â””â”€â”€ docs/                # Full documentation
```

---

## ğŸš€ Quickstart

### 1. Install the framework
```bash
git clone https://github.com/rybalena/multimodal_eval.git
cd multimodal_eval
python3 -m venv .venv && source .venv/bin/activate
pip install -e .
```

### 2. Install model backends (âš ï¸ Required)

MultimodalEvalCore will not work without at least one model installed.  
See the full [Installation Models Guide](docs/installation_models.md), or use quick steps below:

- **LLaVA via Ollama**  
  ```bash
  curl -fsSL https://ollama.com/download | sh
  ollama pull llava:7b    # smaller baseline
  ollama pull llava:14b   # larger benchmark model
  ```

- **Qwen2-VL-2B-Instruct**  
  ```bash
  pip install transformers accelerate
  ```

- **OpenAI GPT-4o**  
  ```bash
  pip install openai
  echo "OPENAI_API_KEY=sk-..." > .env
  ```

âš¡ Do not close Ollama after launching a model (`ollama run llava:7b`) â€” it must stay active while running evaluations.

### 3. Run a task (CLI)
```bash
python -m multimodal_eval.cli.main --task captioning --dataset_type labeled
```

### 4. Run all tasks
```bash
python -m multimodal_eval.cli.run_all
```

### 5. Launch UI
```bash
streamlit run app_ui/streamlit_ui/streamlit_app.py
```

---

## ğŸ“Š Example Output

Results stored in `results/` as JSON/CSV tables.  

**Captioning summary (JSON):**
```json
{
  "task": "captioning",
  "dataset_type": "labeled_data",
  "model_name": "qwen",
  "num_samples": 100,
  "mean_clip_score": 0.74,
  "mean_semantic_similarity": 0.82,
  "mean_cider": 0.69,
  "mean_composite_score": 0.77
}
```

---

## ğŸ“Š Benchmarking & Results Viewer

The framework provides **two ways to benchmark models**:

### 1. Interactive Benchmark (Streamlit UI)
Run:
```bash
streamlit run app_ui/streamlit_ui/streamlit_app.py
```
Features:
- Side-by-side **model comparison** (e.g., Qwen vs LLaVA, GPT-4o vs LLaVA)  
- Choose **task** (captioning, VQA, hallucination, contextual relevance) and **dataset type** (Labeled / Unlabeled)  
- **Tabular benchmark view**: metrics + composite scores directly in the dashboard  
- Immediate feedback for experiments â€” no static HTML needed  

### 2. Results Viewer (History & Comparison)
Run:
```bash
streamlit run app_ui/results_viewer/results_viewer.py
```
Features:
- Loads aggregated runs from `results/all_runs.csv`  
- Displays a **comparison table** of models across multiple runs  
- Useful for **benchmark tracking** and reporting performance trends over time  

Together, these tools turn MultimodalEval into a **full benchmarking suite** for multimodal QA.

---

## ğŸ“¥ Adding Custom Datasets

You can evaluate your own datasets by adding them via the **`user_dataset/`** path.  

### CLI with user datasets
```bash
python -m multimodal_eval.cli.main --task captioning --dataset_type labeled --sources user
```

### Programmatic API

```python
from multimodal_eval.utils.add_dataset import add_dataset

# Add custom dataset
add_dataset("my_dataset.json", dataset_type="labeled", task="captioning")

# Then run evaluation on it
```

This allows benchmarking not only on provided **Labeled** and **Unlabeled** datasets, but also on **your own data**.

---

## ğŸ“˜ Documentation

- [Quickstart](docs/quickstart.md)  
- [Usage Guide](docs/usage.md)  
- [Running Guide](docs/running.md)  
- [Tasks Guide](docs/tasks.md)  
- [Metrics Guide](docs/metrics.md)  
- [Interpreting Results](docs/interpreting_results.md)  
- [Installation Models](docs/installation_models.md)  
- [Tests Guide](docs/tests.md)  
- [Troubleshooting](docs/troubleshooting.md)  
- [Roadmap](docs/roadmap.md)  

---

## ğŸ”® Roadmap Highlights

- **Adversarial Red Teaming** for robustness  
- **Failure-case loop**: fine-tuning on hallucination & irrelevant outputs  
- **Explainability (XAI)** with SHAP/LIME  
- **More models**: BLIP-2, MiniGPT-4, InstructBLIP, Mistral  

---

## ğŸ¤ Contributing

Contributions are welcome: add new metrics, tasks, or model wrappers.  
- Fork â†’ branch â†’ PR  
- For large changes, open an issue first  

---

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE.md)

---

## ğŸ‘©â€ğŸ’» Author

**Elena Rybina**  
AI QA Engineer Â· Computer Vision Enthusiast  
- [LinkedIn](https://www.linkedin.com/in/elena-rybina-5222bb118)  
- [GitHub](https://github.com/rybalena)  
