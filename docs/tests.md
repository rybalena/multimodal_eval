# Testing Guide 

This document explains how to run and extend tests in **MultimodalEval**.

---

## âœ… Requirements

- Python â‰¥ 3.9  
- Install dev dependencies:  
  ```bash
  pip install -r requirements-dev.txt
  ```
  or if using Poetry:
  ```bash
  poetry install --with dev
  ```

---

## â–¶ï¸ Running Tests

We use **pytest** as the testing framework.

Run all tests:
```bash
pytest
```

Run a specific test file:
```bash
pytest tests/test_metrics.py
```

Run a single test case:
```bash
pytest tests/test_metrics.py::test_clip_score
```

Show detailed output:
```bash
pytest -v
```

Generate a coverage report:
```bash
pytest --cov=MultimodalEval
```

---

## ğŸ“‚ Test Structure

```
tests/
 â”œâ”€ test_metrics.py          # Unit tests for evaluation metrics
 â”œâ”€ test_tasks.py            # Validation of task schemas and configs
 â”œâ”€ test_datasets.py         # Dataset loading and schema compliance
 â”œâ”€ test_orchestrator.py     # CLI orchestration and pipeline flow
 â”œâ”€ test_model_wrappers.py   # Mocked tests for model integration (Qwen, LLaVA, GPT-4o)
 â””â”€ conftest.py              # Shared pytest fixtures
```

---

## ğŸ§© Writing New Tests

1. Place the new test file in `tests/` with prefix `test_`.  
2. Use **pytest** functions starting with `test_`. Example:

```python
# tests/test_example.py

from multimodal_eval.metrics import clip_score


def test_clip_score_valid_input():
    score = clip_score("a cat sitting on a mat", "an image of a cat on a mat")
    assert 0.0 <= score <= 1.0
```

3. For new tasks or metrics:  
   - Add fixture data under `tests/resources/`  
   - Test edge cases (empty input, invalid schema, extreme values)

---

## ğŸ” Linting and Type Checking

We recommend running these checks before submitting code:

```bash
black .          # auto-formatting
flake8 .         # style linting
mypy MultimodalEval/   # type checking
```

---

## ğŸ“ Best Practices

- Keep tests **isolated** (no external API calls unless mocked).  
- Cover **both success and failure** cases.  
- Use **fixtures** in `conftest.py` for reusable data.  
- Aim for **high coverage**, but prioritize meaningful tests over numbers.  

---

## ğŸš€ Continuous Integration (Optional)

In GitHub Actions, add this step:

```yaml
- name: Run tests
  run: pytest --cov=MultimodalEval
```

This ensures all PRs are tested automatically.
