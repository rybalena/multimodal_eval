from typing import List

# ---------- SYSTEM PROMPTS ----------

def system_prompt_captioning() -> str:
    return "You are a helpful assistant that evaluates image captions."

def system_prompt_contextual() -> str:
    return "You are a helpful assistant that judges how well an AI response aligns with an image and a prompt."

def system_prompt_hallucination() -> str:
    return "You are a helpful assistant that detects hallucinations and factual issues in captions based on visual object references."


# ---------- PROMPT BUILDERS ----------

def build_caption_eval_prompt(caption: str) -> tuple[str, str]:
    user_prompt = f"""
You are a professional image caption evaluator.

Given:
- An image.
- A caption describing the image, generated by an AI system.

Your task is to rate the caption's quality according to the following 4 categories, with scores from 0.0 (poor) to 1.0 (excellent):

1. image_grounding (weight = 0.35): Does the caption accurately describe the visible content of the image?
2. coverage (weight = 0.25): Does the caption cover all important or salient aspects of the image?
3. fluency (weight = 0.20): Is the caption grammatically correct, coherent, and fluent?
4. hallucination_raw: Are there any elements in the caption that are not present in the image?

Finally, compute:
composite_score = (
    image_grounding * 0.35 +
    coverage * 0.25 +
    fluency * 0.20 +
    (1 - hallucination_raw) * 0.20
)

Return JSON in this format:
{{
  📏"image_grounding": 0.0,
  📏"coverage": 0.0,
  📏"fluency": 0.0,
  📏"hallucination_raw": 0.0,
  ✅"composite_score": 0.0,
  🧠"Reason": "Explain each score briefly."
}}

Caption: "{caption}"
""".strip()

    return user_prompt, system_prompt_captioning()


def build_contextual_prompt(prompt: str, response: str) -> tuple[str, str]:
    user_prompt = f"""
You are an expert evaluator assessing the quality of an AI-generated response to a given prompt and image.

Evaluate the response using the following three categories. Assign each a score between 0.0 (poor) and 1.0 (excellent):

1. image_grounding (weight = 0.4) — Does the response accurately describe and reflect the visual content of the image?
   - 1.0 = All mentioned elements are clearly present in the image
   - 0.5 = Some elements are present, others are vague or omitted
   - 0.0 = Most elements are not in the image

2. prompt_alignment (weight = 0.4) — Does the response properly follow the prompt's instructions and intent?
   - 1.0 = Fully follows the prompt
   - 0.5 = Partially fulfills the prompt
   - 0.0 = Ignores or contradicts the prompt

3. hallucination_raw — Are there any objects, actions, or information in the response that are not visible in the image?
   - 1.0 = No hallucinated elements
   - 0.5 = Minor hallucinations
   - 0.0 = Major hallucinations

Then calculate a composite score using the following formula:
composite_score = image_grounding * 0.4 + prompt_alignment * 0.4 + (1 - hallucination_raw) * 0.2

Return JSON in the following format:

{{
  📏"image_grounding": 0.0,
  📏"prompt_alignment": 0.0,
  📏"hallucination_raw": 0.0,
  ✅"composite_score": 0.0,
  🧠"Reason": "Explain each score individually, based on the image and the prompt."
}}

Prompt: "{prompt}"
Response: "{response}"
""".strip()

    return user_prompt, system_prompt_contextual()


def build_hallucination_prompt(caption: str, ground_truth_objects: List[str]) -> tuple[str, str]:
    user_prompt = f"""
You are a professional multimodal evaluator. Your task is to assess the factual accuracy and semantic quality of a caption generated by an AI model for a given image.

You are provided with:
- An image.
- A caption generated for this image.
- A list of ground truth objects that are visually confirmed to be present in the image.

Please perform the following:

1. Identify any objects mentioned in the caption that are **not present** in the image (`hallucinated_objects`).
2. Identify any **important ground truth objects** from the list that are **missing** in the caption (`missing_objects`).
3. Identify any **semantic inconsistencies or logical errors** in the caption, even if all objects are visually grounded (`semantic_issues`).
4. Assign a **hall_score** from 0.0 to 1.0:
   - 0.0 means the caption is fully accurate and grounded.
   - 1.0 means the caption is highly inaccurate or misleading.
   - Higher values indicate worse factual grounding (more hallucinations or missing key objects).
5. Assign a **semantic_score** from 0.0 to 1.0:
   - 0.0 means the caption is meaningless or logically broken.
   - 1.0 means the caption is fully coherent, logically consistent, and semantically correct.
6. Explain your reasoning clearly and concisely in natural language.

Definitions:
- `hallucinated_objects`: Objects falsely described that do not appear in the image.
- `missing_objects`: Key real objects from the ground truth that the caption failed to mention.
- `semantic_issues`: Logical or linguistic mistakes that make the caption misleading or confusing, even if all objects are technically present.

After that, compute:
- `composite_score` = (semantic_score + (1.0 - hall_score)) / 2

Return your result in **valid JSON** format like this:
{{
  📏"hallucinated_objects": [...],
  📏"missing_objects": [...],
  📏"semantic_issues": [...],
  📏"hall_score": 0.0,
  📏"semantic_score": 0.0,
  ✅"composite_score": 0.0,
  🧠"Reason": "Explain your reasoning here."
}}

Caption: "{caption}"
Ground truth objects in the image: {ground_truth_objects}
""".strip()

    return user_prompt, system_prompt_hallucination()
